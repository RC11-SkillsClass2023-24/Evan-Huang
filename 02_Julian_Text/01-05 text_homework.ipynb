{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5400ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.sparse import random\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import words\n",
    "import gensim\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, stopwords, names\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import random\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import os\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2363a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\76392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\76392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fd9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_book(epub_file, ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    text = \"\"\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            content = item.get_content().decode('utf-8')\n",
    "            content = re.sub('<[^<]+?>', '', content)  # Remove HTML tags\n",
    "            content = re.sub('\\s+', ' ', content)  # Replace multiple whitespaces with a single space\n",
    "            content = re.sub('\\n', ' ', content)\n",
    "            content = re.sub(\"&#13;\", ' ', content)\n",
    "            text += content\n",
    "            #paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    book_item = {'text': text, 'bookID':ID}\n",
    "    return book_item\n",
    "\n",
    "def merge_strings_until_limit(strings, min_length, max_length, test_for_max = 0):\n",
    "    merged_string = \"\"\n",
    "    merged_strings = []\n",
    "\n",
    "    for s in strings:\n",
    "        if len(merged_string) <= min_length:# and len(s)<max_length:\n",
    "            merged_string += s\n",
    "        \n",
    "        elif len(merged_string) > max_length and test_for_max<5:\n",
    "            splitParagraph = merged_string.split('.')\n",
    "            splitParagraphRePoint = []\n",
    "            for sp in splitParagraph:\n",
    "                splitParagraphRePoint.append(sp+'.')\n",
    "            merged = merge_strings_until_limit(splitParagraphRePoint, min_length, max_length, test_for_max+1)\n",
    "            merged_strings.extend(merged)\n",
    "            merged_string = s\n",
    "        else:\n",
    "            merged_strings.append(merged_string)\n",
    "            merged_string = s\n",
    "\n",
    "    if merged_string:\n",
    "        merged_strings.append(merged_string)\n",
    "\n",
    "    return merged_strings\n",
    "\n",
    "def read_epub_paragraphs(epub_file, ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    paragraphs = []\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            content = item.get_content().decode('utf-8')\n",
    "            content = re.sub('<[^<]+?>', '', content)  # Remove HTML tags\n",
    "            content = re.sub('\\s+', ' ', content)  # Replace multiple whitespaces with a single space\n",
    "            content = re.sub('\\n', ' ', content)\n",
    "            paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    paragraphs = merge_strings_until_limit(paragraphs, 200, 1000)\n",
    "    paragraphs = [{'paragraph':paragraphs[i], 'nr':i,'bookID':ID} for i in range(len(paragraphs)) if len(paragraphs[i])>50]\n",
    "    return paragraphs[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1209d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = {}\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        textInfo = page.get_text(\"blocks\", flags=1+2+8)\n",
    "        paragraphs = [re.sub('\\n', ' ', t[4]) for t in textInfo]\n",
    "        text[page_num] = paragraphs\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, image_output_dir):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    images = []\n",
    "\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img_info in enumerate(image_list):\n",
    "            image = pdf_document.extract_image(img_info[0])\n",
    "            image_path = f\"{image_output_dir}/page_{page_num + 1}_image_{img_index}.png\"\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image[\"image\"])\n",
    "\n",
    "            images.append(image_path)\n",
    "\n",
    "    pdf_document.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff658544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraphs):\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "    LEMMATIZER = WordNetLemmatizer()\n",
    "    STEMMER = PorterStemmer()\n",
    "    \n",
    "    processed_docs = [] \n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        words = gensim.utils.simple_preprocess(paragraph, min_len=3, deacc=True)        \n",
    "        lemmatized_words = [LEMMATIZER.lemmatize(word) for word in words]\n",
    "        \n",
    "        filtered_words = [word for word in lemmatized_words if (word not in STOP_WORDS) and (is_english_word(word))]\n",
    "        \n",
    "        stemmed_words = [STEMMER.stem(word) for word in filtered_words]\n",
    "        \n",
    "        processed_doc = \" \".join(stemmed_words)\n",
    "        processed_docs.append(processed_doc)  \n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "def preprocess2(paragraphs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "    processed_docs = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        words = gensim.utils.simple_preprocess(paragraph, min_len=3, deacc=True)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        filtered_words = [word for word in lemmatized_words if word not in STOP_WORDS]\n",
    "        processed_docs.append(filtered_words)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c527ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiple_epub(epub_files):\n",
    "    IDs = [os.path.splitext(os.path.basename(e))[0] for e in epub_files]\n",
    "    all_paragraphs = []\n",
    "    for epub_file, ID in zip(epub_files, IDs):\n",
    "        paragraphs = read_epub_paragraphs(epub_file, ID)\n",
    "        all_paragraphs.extend(paragraphs)\n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27cc39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "\n",
    "def is_english_word(word):\n",
    "    return word in english_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acf17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca12a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_DIR = r'D:\\YeqinHUANG_Collection\\ucl\\week14_28.Jan.2024\\06text\\epub'\n",
    "BOOK_FILES = []\n",
    "BOOK_FILES.extend(glob.glob(os.path.join(BOOK_DIR, '*.epub')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d37ef8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76392\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "PARAGRAPHS = read_multiple_epub(BOOK_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "566fc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Paragraph.obj','wb') as pickleFile:\n",
    "    pickle.dump(PARAGRAPHS, pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c5e3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Paragraph.obj','rb') as pickleFile:\n",
    "    PARAGRAPHS_LOADED = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b864a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED = preprocess([p['paragraph'] for p in PARAGRAPHS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "755d7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED2 = preprocess([p['paragraph'] for p in PARAGRAPHS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c1bb7",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab323882",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER_TFIDF = TfidfVectorizer(min_df=2)\n",
    "TFIDF_MATRIX = VECTORIZER_TFIDF.fit_transform(PREPROCESSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dfb3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tfidf(query, k=1):\n",
    "    model = NearestNeighbors(n_neighbors = k,\n",
    "                            metric = 'cosine',\n",
    "                            algorithm = 'brute',\n",
    "                            n_jobs = -1)\n",
    "    model.fit(TFIDF_MATRIX)\n",
    "    qv = get_tfidf_vector(query)\n",
    "    neighbours = model.kneighbors(qv, k, return_distance = False)[0]\n",
    "    paragraphs = [PARAGRAPHS[i] for i in neighbours]\n",
    "    return paragraphs\n",
    "\n",
    "def get_tfidf_vector(query):\n",
    "    query_vector = VECTORIZER_TFIDF.transform([query])\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d57f028a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': ' “Hello, Jake. Hello!” Mike called. “Come here. I want you to meet my friends. We’re all having an hors–d’œuvre.” I was introduced to the people at the table. They supplied their names to Mike and sent for a fork for me.',\n",
       "  'nr': 927,\n",
       "  'bookID': 'hemingway-sun-also-rises'},\n",
       " {'paragraph': ' I’ve been wanting to talk to you.” “Hello, Frances,” said Cohn. He smiled. “Why, hello, Robert. Are you here?” She went on, talking rapidly. “I’ve had the darndest time. This one”—shaking her head at Cohn—“didn’t come home for lunch.',\n",
       "  'nr': 258,\n",
       "  'bookID': 'hemingway-sun-also-rises'},\n",
       " {'paragraph': ' “Here come the gentry,” Bill said. They were crossing the street “Hello, men,” I said. “Hello, gents!” said Brett. “You saved us seats? How nice.” “I say,” Mike said, “that Romero what’shisname is somebody.',\n",
       "  'nr': 978,\n",
       "  'bookID': 'hemingway-sun-also-rises'},\n",
       " {'paragraph': ' “Have another port?” “All right,” said Harvey. “There comes Cohn,” I said. Robert Cohn was crossing the street. “That moron,” said Harvey. Cohn came up to our table. “Hello, you bums,” he said. “Hello, Robert,” Harvey said.',\n",
       "  'nr': 242,\n",
       "  'bookID': 'hemingway-sun-also-rises'},\n",
       " {'paragraph': ' This whole show makes me sick is all.” Brett came up to the bar. “Hello, you chaps.” “Hello, Brett,” I said. “Why aren’t you tight?” “Never going to get tight any more. I say, give a chap a brandy and soda.',\n",
       "  'nr': 110,\n",
       "  'bookID': 'hemingway-sun-also-rises'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tfidf('hello',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8daec",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bd98e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 250\n",
    "SVD = TruncatedSVD(n_components = n_components, algorithm = 'randomized')\n",
    "REDUCED_MATRIX = SVD.fit_transform(TFIDF_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16fdbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_vector(query):\n",
    "    tfidf_v = get_tfidf_vector(query)\n",
    "    svd_v = SVD.transform(tfidf_v)\n",
    "    return svd_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "583f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_svd(query, k=1):\n",
    "    model = NearestNeighbors(n_neighbors = k,\n",
    "                            metric='cosine',\n",
    "                            algorithm='brute',\n",
    "                            n_jobs=-1)\n",
    "    model.fit(REDUCED_MATRIX)    \n",
    "    qv = get_svd_vector(query)\n",
    "    neighbours = model.kneighbors(qv, k, return_distance=False)[0]\n",
    "    paragraphs = [PARAGRAPHS[i] for i in neighbours]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d433819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_svd_k(query, k=1):\n",
    "    model = NearestNeighbors(n_neighbors = k,\n",
    "                            metric = 'cosine',\n",
    "                            algorithm = 'brute',\n",
    "                            n_jobs = -1)\n",
    "    model.fit(REDUCED_MATRIX)    \n",
    "    qv = get_svd_vector(query)\n",
    "    neighbours = model.kneighbors(qv, k, return_distance = False)[0]\n",
    "    paragraphs = [PARAGRAPHS[i] for i in neighbours]\n",
    "    return paragraphs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "770e01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5496f5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': ' “Hello, Jake. Hello!” Mike called. “Come here. I want you to meet my friends. We’re all having an hors–d’œuvre.” I was introduced to the people at the table. They supplied their names to Mike and sent for a fork for me.',\n",
       "  'nr': 927,\n",
       "  'bookID': 'hemingway-sun-also-rises'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_svd_k(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb6871",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b308171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "DOC2VEC_DOCUMENTS = [TaggedDocument(doc, [i]) for i, doc in enumerate(PREPROCESSED2)]\n",
    "\n",
    "DOC2VEC_MODEL = Doc2Vec(vector_size=250, min_count=0, alpha=0.025, min_alpha=0.025, epochs=100)\n",
    "DOC2VEC_MODEL.build_vocab(DOC2VEC_DOCUMENTS)\n",
    "DOC2VEC_MODEL.train(DOC2VEC_DOCUMENTS, total_examples=DOC2VEC_MODEL.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89f2f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_vector(query):\n",
    "    tokens = preprocess2([query])[0]\n",
    "    vector = DOC2VEC_MODEL.infer_vector(tokens)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1470adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_doc2vec(query, k=1):\n",
    "    vectorQuery = get_doc2vec_vector(query)\n",
    "    neighbors = DOC2VEC_MODEL.dv.most_similar(positive=[vectorQuery], topn=k)\n",
    "    paragraphs = [PARAGRAPHS[n[0]] for n in neighbors]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a56d8b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': ' But this treaty was purchased at an expense which might have supported a vigorous and successful war; and the subjects of Theodosius were compelled to redeem the safety of a worthless favorite by oppressive taxes, which they would more cheerfully have paid for his destruction.',\n",
       "  'nr': 1930,\n",
       "  'bookID': 'gibbon-history-of-the-decline-and-fall-of-the-roman-empire-volume-3'},\n",
       " {'paragraph': ' The impatient crowd rushed at the dawn of day to secure their places, and there were many who passed a sleepless and anxious night in the adjacent porticos. From the morning to the evening, careless of the sun, or of the rain, the spectators, who sometimes amounted to the number of four hundred thousand, remained in eager attention; their eyes fixed on the horses and charioteers, their minds agitated with hope and fear, for the success of the colors which they espoused: and the happiness of Rome appeared to hang on the event of a race.',\n",
       "  'nr': 1082,\n",
       "  'bookID': 'gibbon-history-of-the-decline-and-fall-of-the-roman-empire-volume-3'},\n",
       " {'paragraph': ' In the midst of the public triumph, Constantinople was protected by a strong enclosure of new and more extensive walls; the same vigilant care was applied to restore the fortifications of the Illyrian cities; and a plan was judiciously conceived, which, in the space of seven years, would have secured the command of the Danube, by establishing on that river a perpetual fleet of two hundred and fifty armed vessels.',\n",
       "  'nr': 1557,\n",
       "  'bookID': 'gibbon-history-of-the-decline-and-fall-of-the-roman-empire-volume-3'},\n",
       " {'paragraph': ' His jaw only went further out. Sometimes he turned to smile that toothed, long–jawed, lipless smile when he was called something particularly insulting, and always the pain that any movement produced grew stronger and stronger, until finally his yellow face was parchment color, and after his second bull was dead and the throwing of bread and cushions was over, after he had saluted the President with the same wolf–jawed smile and contemptuous eyes, and handed his sword over the barrera to be wiped, and put back in its case, he passed through into the callejon and leaned on the barrera below us, his head on his arms, not seeing, not hearing anything, only going through his pain.',\n",
       "  'nr': 1286,\n",
       "  'bookID': 'hemingway-sun-also-rises'},\n",
       " {'paragraph': ' The short interval of a perfidious truce was employed to concert more effectual measures for the disgrace and ruin of the archbishop. A numerous council of the Eastern prelates, who were guided from a distance by the advice of Theophilus, confirmed the validity, without examining the justice, of the former sentence; and a detachment of Barbarian troops was introduced into the city, to suppress the emotions of the people.',\n",
       "  'nr': 1528,\n",
       "  'bookID': 'gibbon-history-of-the-decline-and-fall-of-the-roman-empire-volume-3'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_doc2vec(query,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340bd64",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "884db015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAllThree(query):\n",
    "    tfidf = search_tfidf(query)\n",
    "    svd = search_svd(query)\n",
    "    doc2vec = search_doc2vec(query)\n",
    "    \n",
    "    print('TFIDF:{0}\\n SVD: {1}\\n doc2vec:{2}'.format(tfidf, svd, doc2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2816a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'running around university'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc2d005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:[{'paragraph': '” “You haven’t been around much, have you?” “Yes, my dear. I have been around very much. I have been around a very great deal.” “Drink your wine,” said Brett. “We’ve all been around. I dare say Jake here has seen as much as you have.', 'nr': 351, 'bookID': 'hemingway-sun-also-rises'}]\n",
      " SVD: [{'paragraph': '” “You haven’t been around much, have you?” “Yes, my dear. I have been around very much. I have been around a very great deal.” “Drink your wine,” said Brett. “We’ve all been around. I dare say Jake here has seen as much as you have.', 'nr': 351, 'bookID': 'hemingway-sun-also-rises'}]\n",
      " doc2vec:[{'paragraph': ' The sectaries were gradually disqualified from the possession of honorable or lucrative employments; and Theodosius was satisfied with his own justice, when he decreed, that, as the Eunomians distinguished the nature of the Son from that of the Father, they should be incapable of making their wills or of receiving any advantage from testamentary donations.', 'nr': 138, 'bookID': 'gibbon-history-of-the-decline-and-fall-of-the-roman-empire-volume-3'}]\n"
     ]
    }
   ],
   "source": [
    "searchAllThree(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dffed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
